{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5645e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./')\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Input, SpatialDropout1D\n",
    "from keras.layers import LSTM, CuDNNLSTM, Activation\n",
    "from keras.layers import Lambda, Embedding, Conv2D, GlobalMaxPool1D\n",
    "from keras.layers import add, concatenate\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.constraints import MinMaxNorm\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41546902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# from data import MODELS_DIR\n",
    "#MODELS_DIR = 'context_vec/models'\n",
    "#from context_vec.custom_layers import TimestepDropout, Camouflage, Highway, SampledSoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302d661",
   "metadata": {},
   "source": [
    "# smi generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b2751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "\n",
    "class SMIDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Generates data for Keras\"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return int(np.ceil(len(self.indices)/self.batch_size))\n",
    "\n",
    "    def __init__(self, corpus, vocab, sentence_maxlen=100, token_maxlen=50, batch_size=32, shuffle=True, token_encoding='word'):\n",
    "        \"\"\"Compiles a Language Model RNN based on the given parameters\n",
    "        :param corpus: filename of corpus\n",
    "        :param vocab: filename of vocabulary\n",
    "        :param sentence_maxlen: max size of sentence\n",
    "        :param token_maxlen: max size of token in characters\n",
    "        :param batch_size: number of steps at each batch\n",
    "        :param shuffle: True if shuffle at the end of each epoch\n",
    "        :param token_encoding: Encoding of token, either 'word' index or 'char' indices\n",
    "        :return: Nothing\n",
    "        \"\"\"\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.vocab = {line.split()[0]: int(line.split()[1]) for line in open(vocab).readlines()}\n",
    "        self.sent_ids = corpus\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.sentence_maxlen = sentence_maxlen\n",
    "        self.token_maxlen = token_maxlen\n",
    "        self.token_encoding = token_encoding\n",
    "        self.all_lines = open(corpus, mode='r', encoding=\"utf-8\").readlines()\n",
    "        with open(self.corpus) as fp:\n",
    "            self.indices = np.arange(len(fp.readlines()))\n",
    "            # newlines = [index for index in range(0, len(self.indices), 2)]\n",
    "            # self.indices = np.delete(self.indices, newlines)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Read sample sequences\n",
    "        word_indices_batch = np.zeros((len(batch_indices), self.sentence_maxlen), dtype=np.int32)\n",
    "        if self.token_encoding == 'char':\n",
    "            word_char_indices_batch = np.full((len(batch_indices), self.sentence_maxlen, self.token_maxlen), 260, dtype=np.int32)\n",
    "\n",
    "        for i, batch_id in enumerate(batch_indices):\n",
    "            # Read sentence (sample)\n",
    "            word_indices_batch[i] = self.all_lines[batch_id].split()\n",
    "            # word_indices_batch[i] = self.get_token_indices(sent_id=batch_id)\n",
    "            if self.token_encoding == 'char':\n",
    "                word_char_indices_batch[i] = self.get_token_char_indices(sent_id=batch_id)\n",
    "\n",
    "        # Build forward targets\n",
    "        for_word_indices_batch = np.zeros((len(batch_indices), self.sentence_maxlen), dtype=np.int32)\n",
    "\n",
    "        padding = np.zeros((1,), dtype=np.int32)\n",
    "\n",
    "        for i, word_seq in enumerate(word_indices_batch ):\n",
    "            for_word_indices_batch[i] = np.concatenate((word_seq[1:], padding), axis=0)\n",
    "\n",
    "        for_word_indices_batch = for_word_indices_batch[:, :, np.newaxis]\n",
    "\n",
    "        # Build backward targets\n",
    "        back_word_indices_batch = np.zeros((len(batch_indices), self.sentence_maxlen), dtype=np.int32)\n",
    "\n",
    "        for i, word_seq in enumerate(word_indices_batch):\n",
    "            back_word_indices_batch[i] = np.concatenate((padding, word_seq[:-1]), axis=0)\n",
    "\n",
    "        back_word_indices_batch = back_word_indices_batch[:, :, np.newaxis]\n",
    "\n",
    "        tmp = [word_indices_batch if 'word'.__eq__(self.token_encoding) else word_char_indices_batch, for_word_indices_batch, back_word_indices_batch]\n",
    "        return tmp, []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def get_token_indices(self, sent_id: int):\n",
    "        with open(self.corpus) as fp:\n",
    "            for i, line in enumerate(fp):\n",
    "                if i == sent_id:\n",
    "                    token_ids = np.zeros((self.sentence_maxlen,), dtype=np.int64)\n",
    "                    # Add begin of sentence index\n",
    "                    token_ids[0] = self.vocab['<bos>']\n",
    "                    for j, token in enumerate(line.split()[:self.sentence_maxlen - 2]):\n",
    "                        # print(token)\n",
    "                        if token.lower() in self.vocab:\n",
    "                            token_ids[j + 1] = self.vocab[token.lower()]\n",
    "                        else:\n",
    "                            token_ids[j + 1] = self.vocab['<unk>']\n",
    "                    # Add end of sentence index\n",
    "                    if token_ids[1]:\n",
    "                        token_ids[j + 2] = self.vocab['<eos>']\n",
    "                    # print(token_ids)\n",
    "                    return token_ids\n",
    "\n",
    "    def get_token_char_indices(self, sent_id: int):\n",
    "        def convert_token_to_char_ids(token, token_maxlen):\n",
    "            bos_char = 256  # <begin sentence>\n",
    "            eos_char = 257  # <end sentence>\n",
    "            bow_char = 258  # <begin word>\n",
    "            eow_char = 259  # <end word>\n",
    "            pad_char = 260  # <pad char>\n",
    "            char_indices = np.full([token_maxlen], pad_char, dtype=np.int32)\n",
    "            # Encode word to UTF-8 encoding\n",
    "            word_encoded = token.encode('utf-8', 'ignore')[:(token_maxlen - 2)]\n",
    "            # Set characters encodings\n",
    "            # Add begin of word char index\n",
    "            char_indices[0] = bow_char\n",
    "            if token == '<bos>':\n",
    "                char_indices[1] = bos_char\n",
    "                k = 1\n",
    "            elif token == '<eos>':\n",
    "                char_indices[1] = eos_char\n",
    "                k = 1\n",
    "            else:\n",
    "                # Add word char indices\n",
    "                for k, chr_id in enumerate(word_encoded, start=1):\n",
    "                    char_indices[k] = chr_id + 1\n",
    "            # Add end of word char index\n",
    "            char_indices[k + 1] = eow_char\n",
    "            return char_indices\n",
    "\n",
    "        with open(self.corpus) as fp:\n",
    "            for i, line in enumerate(fp):\n",
    "                if i == sent_id:\n",
    "                    token_ids = np.zeros((self.sentence_maxlen, self.token_maxlen), dtype=np.int32)\n",
    "                    # Add begin of sentence char indices\n",
    "                    token_ids[0] = convert_token_to_char_ids('<bos>', self.token_maxlen)\n",
    "                    # Add tokens' char indices\n",
    "                    for j, token in enumerate(line.split()[:self.sentence_maxlen - 2]):\n",
    "                        token_ids[j + 1] = convert_token_to_char_ids(token, self.token_maxlen)\n",
    "                    # Add end of sentence char indices\n",
    "                    if token_ids[1]:\n",
    "                        token_ids[j + 2] = convert_token_to_char_ids('<eos>', self.token_maxlen)\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36b967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12784a72",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d478e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context_vec(object):\n",
    "    def __init__(self, parameters):\n",
    "        self._model = None\n",
    "        self._context_vec_model = None\n",
    "        self.parameters = parameters\n",
    "        self.model_dir = parameters['model_dir']\n",
    "        if not os.path.exists(os.path.join(MODELS_DIR, self.model_dir)):\n",
    "            os.mkdir(os.path.join(MODELS_DIR, self.model_dir))\n",
    "        self.compile_context_vec()\n",
    "\n",
    "    def __del__(self):\n",
    "        K.clear_session()\n",
    "        del self._model\n",
    "\n",
    "    def char_level_token_encoder(self):\n",
    "        charset_size = self.parameters['charset_size']\n",
    "        char_embedding_size = self.parameters['char_embedding_size']\n",
    "        token_embedding_size = self.parameters['hidden_units_size']\n",
    "        n_highway_layers = self.parameters['n_highway_layers']\n",
    "        filters = self.parameters['cnn_filters']\n",
    "        token_maxlen = self.parameters['token_maxlen']\n",
    "\n",
    "        # Input Layer, word characters (samples, words, character_indices)\n",
    "        inputs = Input(shape=(None, token_maxlen,), dtype='int32')\n",
    "        # Embed characters (samples, words, characters, character embedding)\n",
    "        embeds = Embedding(input_dim=charset_size, output_dim=char_embedding_size)(inputs)\n",
    "        token_embeds = []\n",
    "        # Apply multi-filter 2D convolutions + 1D MaxPooling + tanh\n",
    "        for (window_size, filters_size) in filters:\n",
    "            convs = Conv2D(filters=filters_size, kernel_size=[window_size, char_embedding_size], strides=(1, 1),\n",
    "                           padding=\"same\")(embeds)\n",
    "            convs = TimeDistributed(GlobalMaxPool1D())(convs)\n",
    "            convs = Activation('tanh')(convs)\n",
    "            convs = Camouflage(mask_value=0)(inputs=[convs, inputs])\n",
    "            token_embeds.append(convs)\n",
    "        token_embeds = concatenate(token_embeds)\n",
    "        \n",
    "        # Apply highways networks\n",
    "        for i in range(n_highway_layers):\n",
    "            token_embeds = TimeDistributed(Highway())(token_embeds)\n",
    "            token_embeds = Camouflage(mask_value=0)(inputs=[token_embeds, inputs])\n",
    "        # Project to token embedding dimensionality\n",
    "        token_embeds = TimeDistributed(Dense(units=token_embedding_size, activation='linear'))(token_embeds)\n",
    "        token_embeds = Camouflage(mask_value=0)(inputs=[token_embeds, inputs])\n",
    "\n",
    "        token_encoder = Model(inputs=inputs, outputs=token_embeds, name='token_encoding')\n",
    "        return token_encoder\n",
    "\n",
    "    def compile_context_vec(self, print_summary=False):\n",
    "        \"\"\"\n",
    "        Compiles a Language Model RNN based on the given parameters\n",
    "        \"\"\"\n",
    "\n",
    "        if self.parameters['token_encoding'] == 'word':\n",
    "            # Train word embeddings from scratch\n",
    "            word_inputs = Input(shape=(None,), name='word_indices', dtype='int32')\n",
    "            embeddings = Embedding(self.parameters['vocab_size'], self.parameters['hidden_units_size'], trainable=True, name='token_encoding')\n",
    "            inputs = embeddings(word_inputs)\n",
    "\n",
    "            # Token embeddings for Input\n",
    "            drop_inputs = SpatialDropout1D(self.parameters['dropout_rate'])(inputs)\n",
    "            lstm_inputs = TimestepDropout(self.parameters['word_dropout_rate'])(drop_inputs)\n",
    "\n",
    "            # Pass outputs as inputs to apply sampled softmax\n",
    "            next_ids = Input(shape=(None, 1), name='next_ids', dtype='float32')\n",
    "            previous_ids = Input(shape=(None, 1), name='previous_ids', dtype='float32')\n",
    "        elif self.parameters['token_encoding'] == 'char':\n",
    "            # Train character-level representation\n",
    "            word_inputs = Input(shape=(None, self.parameters['token_maxlen'],), dtype='int32', name='char_indices')\n",
    "            inputs = self.char_level_token_encoder()(word_inputs)\n",
    "\n",
    "            # Token embeddings for Input\n",
    "            drop_inputs = SpatialDropout1D(self.parameters['dropout_rate'])(inputs)\n",
    "            lstm_inputs = TimestepDropout(self.parameters['word_dropout_rate'])(drop_inputs)\n",
    "\n",
    "            # Pass outputs as inputs to apply sampled softmax\n",
    "            next_ids = Input(shape=(None, 1), name='next_ids', dtype='float32')\n",
    "            previous_ids = Input(shape=(None, 1), name='previous_ids', dtype='float32')\n",
    "\n",
    "        # Reversed input for backward LSTMs\n",
    "        re_lstm_inputs = Lambda(function=Context_vec.reverse)(lstm_inputs)\n",
    "        mask = Lambda(function=Context_vec.reverse)(drop_inputs)\n",
    "\n",
    "        # Forward LSTMs\n",
    "        for i in range(self.parameters['n_lstm_layers']):\n",
    "            lstm = LSTM(units=self.parameters['lstm_units_size'], return_sequences=True, activation=\"tanh\",\n",
    "                        recurrent_activation='sigmoid',\n",
    "                        kernel_constraint=MinMaxNorm(-1 * self.parameters['cell_clip'],\n",
    "                                                     self.parameters['cell_clip']),\n",
    "                        recurrent_constraint=MinMaxNorm(-1 * self.parameters['cell_clip'],\n",
    "                                                        self.parameters['cell_clip'])\n",
    "                        )(lstm_inputs)\n",
    "            lstm = Camouflage(mask_value=0)(inputs=[lstm, drop_inputs])\n",
    "            # Projection to hidden_units_size\n",
    "            proj = TimeDistributed(Dense(self.parameters['hidden_units_size'], activation='linear',\n",
    "                                         kernel_constraint=MinMaxNorm(-1 * self.parameters['proj_clip'],\n",
    "                                                                      self.parameters['proj_clip'])\n",
    "                                         ))(lstm)\n",
    "            # Merge Bi-LSTMs feature vectors with the previous ones\n",
    "            lstm_inputs = add([proj, lstm_inputs], name='f_block_{}'.format(i + 1))\n",
    "            # Apply variational drop-out between BI-LSTM layers\n",
    "            lstm_inputs = SpatialDropout1D(self.parameters['dropout_rate'])(lstm_inputs)\n",
    "\n",
    "        # Backward LSTMs\n",
    "        for i in range(self.parameters['n_lstm_layers']):\n",
    "            re_lstm = LSTM(units=self.parameters['lstm_units_size'], return_sequences=True, activation='tanh',\n",
    "                           recurrent_activation='sigmoid',\n",
    "                           kernel_constraint=MinMaxNorm(-1 * self.parameters['cell_clip'],\n",
    "                                                        self.parameters['cell_clip']),\n",
    "                           recurrent_constraint=MinMaxNorm(-1 * self.parameters['cell_clip'],\n",
    "                                                           self.parameters['cell_clip'])\n",
    "                           )(re_lstm_inputs)\n",
    "            re_lstm = Camouflage(mask_value=0)(inputs=[re_lstm, mask])\n",
    "            # Projection to hidden_units_size\n",
    "            re_proj = TimeDistributed(Dense(self.parameters['hidden_units_size'], activation='linear',\n",
    "                                            kernel_constraint=MinMaxNorm(-1 * self.parameters['proj_clip'],\n",
    "                                                                         self.parameters['proj_clip'])\n",
    "                                            ))(re_lstm)\n",
    "            # Merge Bi-LSTMs feature vectors with the previous ones\n",
    "            re_lstm_inputs = add([re_proj, re_lstm_inputs], name='b_block_{}'.format(i + 1))\n",
    "            # Apply variational drop-out between BI-LSTM layers\n",
    "            re_lstm_inputs = SpatialDropout1D(self.parameters['dropout_rate'])(re_lstm_inputs)\n",
    "\n",
    "        # Reverse backward LSTMs' outputs = Make it forward again\n",
    "        re_lstm_inputs = Lambda(function=Context_vec.reverse, name=\"reverse\")(re_lstm_inputs)\n",
    "\n",
    "        # Project to Vocabulary with Sampled Softmax\n",
    "        sampled_softmax = SampledSoftmax(num_classes=self.parameters['vocab_size'],\n",
    "                                         num_sampled=int(self.parameters['num_sampled']),\n",
    "                                         tied_to=embeddings if self.parameters['weight_tying']\n",
    "                                         and self.parameters['token_encoding'] == 'word' else None)\n",
    "        outputs = sampled_softmax([lstm_inputs, next_ids])\n",
    "        re_outputs = sampled_softmax([re_lstm_inputs, previous_ids])\n",
    "\n",
    "        self._model = Model(inputs=[word_inputs, next_ids, previous_ids],\n",
    "                            outputs=[outputs, re_outputs])\n",
    "        self._model.compile(optimizer=Adagrad(lr=self.parameters['lr'], clipvalue=self.parameters['clip_value']),\n",
    "                            loss=None)\n",
    "        if print_summary:\n",
    "            self._model.summary()\n",
    "\n",
    "    def train(self, train_data, valid_data):\n",
    "\n",
    "        # Add callbacks (early stopping, model checkpoint)\n",
    "        weights_file = os.path.join(MODELS_DIR, self.model_dir, \"context_vec_best_weights_{epoch:03d}_{val_loss:.2f}.hdf5\")\n",
    "        save_best_model = ModelCheckpoint(filepath=weights_file, monitor='val_loss', verbose=1,\n",
    "                                          save_best_only=False, mode='auto')\n",
    "        # early_stopping = EarlyStopping(patience=self.parameters['patience'], restore_best_weights=True)\n",
    "\n",
    "        t_start = time.time()\n",
    "\n",
    "        # Fit Model\n",
    "        self._model.fit_generator(train_data,\n",
    "                                  validation_data=valid_data,\n",
    "                                  epochs=self.parameters['epochs'],\n",
    "                                  workers=self.parameters['n_threads']\n",
    "                                  if self.parameters['n_threads'] else os.cpu_count(),\n",
    "                                  use_multiprocessing=True\n",
    "                                  if self.parameters['multi_processing'] else False,\n",
    "                                  callbacks=[save_best_model])\n",
    "\n",
    "        print('Training took {0} sec'.format(str(time.time() - t_start)))\n",
    "\n",
    "    def evaluate(self, test_data, batch_size):\n",
    "        # 查找完整句子\n",
    "        def unpad(x, y_true, y_pred):\n",
    "            y_true_unpad = []\n",
    "            y_pred_unpad = []\n",
    "            for i, x_i in enumerate(x):\n",
    "                for j, x_ij in enumerate(x_i):\n",
    "                    if x_ij == 0:\n",
    "                        y_true_unpad.append(y_true[i][:j])\n",
    "                        y_pred_unpad.append(y_pred[i][:j])\n",
    "                        break\n",
    "            return np.asarray(y_true_unpad), np.asarray(y_pred_unpad)\n",
    "\n",
    "        # # Generate samples\n",
    "        # x, y_true_forward, y_true_backward = [], [], []\n",
    "        # for i in range(len(test_data)):\n",
    "        #     test_batch = test_data[i][0]\n",
    "        #     x.extend(test_batch[0])\n",
    "        #     y_true_forward.extend(test_batch[1])\n",
    "        #     y_true_backward.extend(test_batch[2])\n",
    "        # x = np.asarray(x)\n",
    "        # y_true_forward = np.asarray(y_true_forward)\n",
    "        # y_true_backward = np.asarray(y_true_backward)\n",
    "\n",
    "        # Generate samples\n",
    "        # x, y_true_forward, y_true_backward = [], [], []\n",
    "        # y_pred_forward, y_pred_backward = [], []\n",
    "        # for i in range(len(test_data)):\n",
    "        for i in range(488):\n",
    "            test_batch = test_data[i][0]\n",
    "\n",
    "            # Predict outputs\n",
    "            y_pred_forward_tmp, y_pred_backward_tmp = self._model.predict([test_batch[0], test_batch[1], test_batch[2]])\n",
    "\n",
    "            # Unpad sequences\n",
    "            y_true_forward, y_pred_forward = unpad(test_batch[0], test_batch[1], y_pred_forward_tmp)\n",
    "            y_true_backward, y_pred_backward = unpad(test_batch[0], test_batch[2], y_pred_backward_tmp)\n",
    "\n",
    "            # Compute and print perplexity\n",
    "            # print('{}， Forward Langauge Model Perplexity: {}'.format(i, context_vec.perplexity(y_pred_forward, y_true_forward)))\n",
    "            # print('{}， Backward Langauge Model Perplexity: {}'.format(i, context_vec.perplexity(y_pred_backward, y_true_backward)))\n",
    "            print('{}， avg {}'.format(i,\n",
    "                                      (Context_vec.perplexity(y_pred_backward, y_true_backward) + Context_vec.perplexity(y_pred_forward, y_true_forward)) / 2\n",
    "                                      ))\n",
    "            del test_batch, y_true_backward, y_pred_backward, \\\n",
    "                y_pred_forward_tmp, y_pred_backward_tmp, y_true_forward, y_pred_forward\n",
    "            gc.collect()\n",
    "            # x.extend(test_batch[0])\n",
    "        #     y_true_forward.extend(test_batch[1])\n",
    "        #     y_true_backward.extend(test_batch[2])\n",
    "        #\n",
    "        #     y_pred_forward.extend(y_pred_forward_tmp)\n",
    "        #     y_pred_backward.extend(y_pred_backward_tmp)\n",
    "        #\n",
    "        # x = np.asarray(x)\n",
    "        # y_true_forward = np.asarray(y_true_forward)\n",
    "        # y_true_backward = np.asarray(y_true_backward)\n",
    "        #\n",
    "        # y_pred_forward = np.asarray(y_pred_forward)\n",
    "        # y_pred_backward = np.asarray(y_pred_backward)\n",
    "\n",
    "        # # Unpad sequences\n",
    "        # y_true_forward, y_pred_forward = unpad(x, y_true_forward, y_pred_forward)\n",
    "        # y_true_backward, y_pred_backward = unpad(x, y_true_backward, y_pred_backward)\n",
    "\n",
    "\n",
    "\n",
    "    def wrap_multi_context_vec_encoder(self, print_summary=False, save=False):\n",
    "        \"\"\"\n",
    "        Wrap context_vec meta-model encoder, which returns an array of the 3 intermediate context_vec outputs\n",
    "        :param print_summary: print a summary of the new architecture\n",
    "        :param save: persist model\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        context_vec_embeddings = list()\n",
    "        context_vec_embeddings.append(concatenate([self._model.get_layer('token_encoding').output, self._model.get_layer('token_encoding').output],\n",
    "                                           name='context_vec_embeddings_level_0'))\n",
    "        for i in range(self.parameters['n_lstm_layers']):\n",
    "            context_vec_embeddings.append(concatenate([self._model.get_layer('f_block_{}'.format(i + 1)).output,\n",
    "                                                Lambda(function=Context_vec.reverse)\n",
    "                                                (self._model.get_layer('b_block_{}'.format(i + 1)).output)],\n",
    "                                               name='context_vec_embeddings_level_{}'.format(i + 1)))\n",
    "\n",
    "        camos = list()\n",
    "        for i, context_vec_embedding in enumerate(context_vec_embeddings):\n",
    "            camos.append(Camouflage(mask_value=0.0, name='camo_context_vec_embeddings_level_{}'.format(i + 1))([context_vec_embedding,\n",
    "                                                                                                         self._model.get_layer(\n",
    "                                                                                                             'token_encoding').output]))\n",
    "\n",
    "        self._context_vec_model = Model(inputs=[self._model.get_layer('word_indices').input], outputs=camos)\n",
    "\n",
    "        if print_summary:\n",
    "            self._context_vec_model.summary()\n",
    "\n",
    "        if save:\n",
    "            self._context_vec_model.save(os.path.join(MODELS_DIR, self.model_dir, 'context_vec_Encoder.hd5'))\n",
    "            print('context_vec Encoder saved successfully')\n",
    "\n",
    "    def save(self, sampled_softmax=True, model_dir=\"model\"):\n",
    "        \"\"\"\n",
    "        Persist model in disk\n",
    "        :param sampled_softmax: reload model using the full softmax function\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if not sampled_softmax:\n",
    "            self.parameters['num_sampled'] = self.parameters['vocab_size']\n",
    "        self.compile_context_vec()\n",
    "        best_name = \"\"\n",
    "        min_loss = 100000\n",
    "        for file_name in os.listdir(os.path.join(MODELS_DIR, self.model_dir)):\n",
    "            if \"0.\" in file_name:\n",
    "                tmp_loss = float(file_name.split(\"_\")[-1].split(\".\")[-2])\n",
    "                if tmp_loss < min_loss:\n",
    "                    min_loss = tmp_loss\n",
    "                    best_name = file_name\n",
    "        shutil.copyfile(os.path.join(MODELS_DIR, self.model_dir, best_name),\n",
    "                        os.path.join(MODELS_DIR, self.model_dir, 'context_vec_best_weights.hdf5'))\n",
    "        print(\" best model name is :\" + best_name)\n",
    "        self._model.load_weights(os.path.join(MODELS_DIR, self.model_dir, 'context_vec_best_weights.hdf5'))\n",
    "        self._model.save(os.path.join(MODELS_DIR, model_dir, 'context_vec_LM_EVAL.hd5'))\n",
    "        print('context_vec Language Model saved successfully')\n",
    "\n",
    "    def load(self, sampled_softmax=False):\n",
    "        if not sampled_softmax:\n",
    "            self.parameters['num_sampled'] = self.parameters['vocab_size']\n",
    "        self.compile_context_vec()\n",
    "        self._model.load_weights(os.path.join(MODELS_DIR, self.model_dir, 'context_vec_best_weights.hdf5'))\n",
    "        # self._model = load_model(os.path.join(MODELS_DIR, self.model_dir, 'context_vec_Encoder.hd5'),\n",
    "        #                          custom_objects={'TimestepDropout': TimestepDropout,\n",
    "        #                                          'Camouflage': Camouflage})\n",
    "\n",
    "    def load_context_vec_encoder(self):\n",
    "        self._context_vec_model = load_model(os.path.join(MODELS_DIR, self.model_dir, 'context_vec_Encoder.hd5'),\n",
    "                                      custom_objects={'TimestepDropout': TimestepDropout,\n",
    "                                                      'Camouflage': Camouflage})\n",
    "\n",
    "    def get_outputs(self, test_data, output_type='word', state='last'):\n",
    "        \"\"\"\n",
    "       Wrap context_vec meta-model encoder, which returns an array of the 3 intermediate context_vec outputs\n",
    "       :param test_data: data generator\n",
    "       :param output_type: \"word\" for word vectors or \"sentence\" for sentence vectors\n",
    "       :param state: 'last' for 2nd LSTMs outputs or 'mean' for mean-pooling over inputs, 1st LSTMs and 2nd LSTMs\n",
    "       :return: None\n",
    "       \"\"\"\n",
    "        # Generate samples\n",
    "        preds1 = []\n",
    "        # for i in range(len(test_data)):\n",
    "        for i in range(10):\n",
    "            x = test_data[i][0]\n",
    "            # print(x[0])\n",
    "            tmp = np.asarray(self._context_vec_model.predict(np.asarray(x[0]))).swapaxes(0,1)\n",
    "            preds1.extend(tmp)\n",
    "            del tmp\n",
    "\n",
    "        preds = np.array(preds1)\n",
    "        del preds1\n",
    "        if state == 'last':\n",
    "            context_vec_vectors = preds[:,-1,:,:]\n",
    "        elif state == 'all':\n",
    "            context_vec_vectors = preds\n",
    "        else:\n",
    "            context_vec_vectors = np.mean(preds, axis=1)\n",
    "\n",
    "        if output_type == 'word':\n",
    "            return context_vec_vectors\n",
    "        else:\n",
    "            return np.mean(context_vec_vectors, axis=2)\n",
    "\n",
    "    def get_outputs_Bylist(self, test_data, output_typeBy='word', state='last'):\n",
    "        \"\"\"\n",
    "       Wrap context_vec meta-model encoder, which returns an array of the 3 intermediate context_vec outputs\n",
    "       :param test_data: data generator\n",
    "       :param output_type: \"word\" for word vectors or \"sentence\" for sentence vectors\n",
    "       :param state: 'last' for 2nd LSTMs outputs or 'mean' for mean-pooling over inputs, 1st LSTMs and 2nd LSTMs\n",
    "       :return: None\n",
    "       \"\"\"\n",
    "        # Generate samples\n",
    "        preds = []\n",
    "        for i in range(len(test_data)):\n",
    "            x = test_data[i][0]\n",
    "            # print(x[0])\n",
    "            tmp = np.asarray(self._context_vec_model.predict(np.asarray(x[0]))).swapaxes(0,1)\n",
    "            preds.extend(tmp)\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def get_predict(self, test_data, output_type='word', state='last'):\n",
    "        # x = []\n",
    "        # for i in range(len(test_data)):\n",
    "        #     test_batch = test_data[0]\n",
    "        #     x.extend(test_batch[0])\n",
    "\n",
    "        preds = np.asarray(self._context_vec_model.predict(np.asarray(test_data)))\n",
    "        if state == 'last':\n",
    "            context_vec_vectors = preds[-1]\n",
    "        else:\n",
    "            context_vec_vectors = np.mean(preds, axis=0)\n",
    "\n",
    "        if output_type == 'words':\n",
    "            return context_vec_vectors\n",
    "        else:\n",
    "            return np.mean(context_vec_vectors, axis=1)\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def reverse(inputs, axes=1):\n",
    "        return K.reverse(inputs, axes=axes)\n",
    "\n",
    "    @staticmethod\n",
    "    def perplexity(y_pred, y_true):\n",
    "        if len(y_pred) == 0 or len(y_true) == 0:\n",
    "            return -1\n",
    "        cross_entropies = []\n",
    "        for y_pred_seq, y_true_seq in zip(y_pred, y_true):\n",
    "            # Reshape targets to one-hot vectors\n",
    "            y_true_seq = to_categorical(y_true_seq, y_pred_seq.shape[-1])\n",
    "            # Compute cross_entropy for sentence words\n",
    "            cross_entropy = K.categorical_crossentropy(K.tf.convert_to_tensor(y_true_seq, dtype=K.tf.float32),\n",
    "                                                       K.tf.convert_to_tensor(y_pred_seq, dtype=K.tf.float32))\n",
    "            cross_entropies.extend(cross_entropy.eval(session=K.get_session()))\n",
    "\n",
    "        # Compute mean cross_entropy and perplexity\n",
    "        cross_entropy = np.mean(np.asarray(cross_entropies), axis=-1)\n",
    "\n",
    "        return pow(2.0, cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d4166",
   "metadata": {},
   "source": [
    "# get_tox_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "881b6cab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_14572\\1118483643.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./')\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# NR-AR\tNR-AR-LBD\tNR-AhR\tNR-Aromatase\n",
    "# NR-ER\tNR-ER-LBD\tNR-PPAR-gamma\tSR-ARE\n",
    "# SR-ATAD5\tSR-HSE\tSR-MMP\tSR-p53\n",
    "dict_label = {\"NR-AR\":0,\n",
    "              \"NR-AR-LBD\":1,\n",
    "              \"NR-AhR\":2,\n",
    "              \"NR-Aromatase\":3,\n",
    "              \"NR-ER\":4,\n",
    "              \"NR-ER-LBD\":5,\n",
    "              \"NR-PPAR-gamma\":6,\n",
    "              \"SR-ARE\":7,\n",
    "              \"SR-ATAD5\":8,\n",
    "              \"SR-HSE\":9,\n",
    "              \"SR-MMP\":10,\n",
    "              \"SR-p53\":11,}\n",
    "\n",
    "\n",
    "# step 1\n",
    "# tox21.csv에서 smiles 문자열 추출\n",
    "filepath=\"C:/Users/Administrator/Desktop/tox21/tox21.csv\"\n",
    "df = pd.read_csv(filepath, header=0, encoding=\"gbk\")\n",
    "all_label = []\n",
    "all_smi = []\n",
    "w_file = open(\"tox/tox.smi\", mode='w',encoding=\"utf-8\")\n",
    "for line in df.values:\n",
    "    smi = line[13].strip()\n",
    "    if len(smi) <= 0:\n",
    "        break\n",
    "    all_label.append(line[:12])\n",
    "    all_smi.append(smi)\n",
    "    w_file.write(smi+\"\\n\")\n",
    "w_file.close()\n",
    "\n",
    "\n",
    "# step 2\n",
    "#mol2vec 임베딩 생성\n",
    "#mol2vec 패키지로 smiles문자열 -> mol2vec 임베딩으로 변환.??\n",
    "adb = \"mol2vec corpus -i tox/tox.smi -o tox/tox.cp -r 1 -j 4 --uncommon UNK --threshold 3\"\n",
    "d = os.popen(adb)\n",
    "f = d.read()\n",
    "print(f)\n",
    "\n",
    "\n",
    "# step 3\n",
    "#train한 vocab\n",
    "vocab_path = \"data/datasets/my_smi/smi_tran.vocab\"\n",
    "vocab = {line.split()[0]: int(line.split()[1]) for line in open(vocab_path).readlines()}\n",
    "\n",
    "sentence_maxlen = 80\n",
    "\n",
    "w_file = open(\"tox/tox_tran.cp_UNK\", mode='w', encoding=\"utf-8\")\n",
    "label = []\n",
    "smi = []\n",
    "index = -1\n",
    "mols_path = \"tox/tox.cp_UNK\"\n",
    "mols_file = open(mols_path, mode='r',encoding=\"utf-8\")\n",
    "while True:\n",
    "    line = mols_file.readline().strip()\n",
    "    index += 1\n",
    "    if \"None\".__eq__(line.strip()) or \"UNK\".__eq__(line.strip()):\n",
    "        continue\n",
    "    if not line:\n",
    "        break\n",
    "    token_ids = np.zeros((sentence_maxlen,), dtype=np.int64)\n",
    "    # Add begin of sentence index\n",
    "    token_ids[0] = vocab['<bos>']\n",
    "    for j, token in enumerate(line.split()[:sentence_maxlen - 2]):\n",
    "        # print(token)\n",
    "        if token.lower() in vocab:\n",
    "            token_ids[j + 1] = vocab[token.lower()]\n",
    "        else:\n",
    "            token_ids[j + 1] = vocab['<unk>']\n",
    "    # Add end of sentence index\n",
    "    if token_ids[1]:\n",
    "        token_ids[j + 2] = vocab['<eos>']\n",
    "    # print(token_ids)\n",
    "    label.append(all_label[index])\n",
    "    smi.append(all_smi[index])\n",
    "    w_file.write(\" \".join(str(i) for i in token_ids).strip()+\"\\n\")\n",
    "w_file.close()\n",
    "\n",
    "joblib.dump(label, \"tox/label.pkl\")\n",
    "joblib.dump(smi, \"tox/smi.pkl\")\n",
    "\n",
    "# step 4\n",
    "import os\n",
    "import keras.backend as K\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# from data import DATA_SET_DIR\n",
    "from context_vec.smi_generator import SMIDataGenerator\n",
    "from context_vec.smi_model import context_vec\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = False\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "parameters = {\n",
    "    'multi_processing': False,\n",
    "    'n_threads': 4,\n",
    "    'cuDNN': False,\n",
    "    'test_dataset': \"tox/tox_tran.cp_UNK\",\n",
    "    'vocab': 'my_smi/smi_tran.vocab',\n",
    "    'model_dir': \"smi_context_vec_best\",\n",
    "    'vocab_flag': False,\n",
    "    'uncommon_threshold': 3,\n",
    "    # 'vocab_size': 28914,\n",
    "    # 'vocab_size': 748,\n",
    "    'vocab_size': 13576,\n",
    "    # 'vocab_size': 121,\n",
    "    'num_sampled': 100,\n",
    "    # 'charset_size': 262,\n",
    "    'sentence_maxlen': 80,\n",
    "    'token_maxlen': 50,\n",
    "    'token_encoding': 'word',\n",
    "    'epochs': 1000,\n",
    "    'patience': 2,\n",
    "    'batch_size': 512,\n",
    "    'test_batch_size': 512,\n",
    "    'clip_value': 1,\n",
    "    'cell_clip': 5,\n",
    "    'proj_clip': 5,\n",
    "    'lr': 0.2,\n",
    "    'shuffle': False,\n",
    "    'n_lstm_layers': 2,\n",
    "    'n_highway_layers': 2,\n",
    "    'cnn_filters': [[1, 32],\n",
    "                    [2, 32],\n",
    "                    [3, 64],\n",
    "                    [4, 128],\n",
    "                    [5, 256],\n",
    "                    [6, 512],\n",
    "                    [7, 512]\n",
    "                    ],\n",
    "    'lstm_units_size': 300,\n",
    "    'hidden_units_size': 150,\n",
    "    'char_embedding_size': 16,\n",
    "    'dropout_rate': 0.1,\n",
    "    'word_dropout_rate': 0.05,\n",
    "    'weight_tying': True,\n",
    "}\n",
    "\n",
    "test_generator = SMIDataGenerator(parameters['test_dataset'],\n",
    "                                os.path.join(\"data/datasets\", parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['test_batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])\n",
    "\n",
    "# Compile context_vec\n",
    "context_vec_model = context_vec(parameters)\n",
    "context_vec_model.compile_context_vec()\n",
    "\n",
    "# context_vec_model.load(sampled_softmax=False)\n",
    "#\n",
    "# # Evaluate Bidirectional Language Model\n",
    "# context_vec_model.evaluate(test_generator, parameters['test_batch_size'])\n",
    "#\n",
    "# # Build context_vec meta-model to deploy for production and persist in disk\n",
    "# context_vec_model.wrap_multi_context_vec_encoder(print_summary=True)\n",
    "\n",
    "# Load context_vec encoder\n",
    "context_vec_model.load_context_vec_encoder()\n",
    "\n",
    "# Get context_vec embeddings to feed as inputs for downstream tasks\n",
    "context_vec_embeddings = context_vec_model.get_outputs(test_generator, output_type='word', state='all')\n",
    "print(context_vec_embeddings.shape)\n",
    "\n",
    "# 保存x\n",
    "joblib.dump(context_vec_embeddings, \"tox/tox_embed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ab076",
   "metadata": {},
   "source": [
    "# tox train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from sklearn import metrics\n",
    "# from utils.util import *\n",
    "# from utils.model import *\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"搭建rnn网络\"\"\"\n",
    "    def __init__(self, out_num, input_size=300, task_type='sing', att=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.matrix = nn.Parameter(torch.tensor([0.33, 0.33, 0.33]), requires_grad=True)\n",
    "        self.input_size = input_size\n",
    "        self.out_num = out_num * 2 if \"muti\".__eq__(task_type) else out_num\n",
    "        self.att = att\n",
    "\n",
    "        self.fc = nn.Linear(self.input_size, 1024)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1024,\n",
    "            hidden_size=1024,\n",
    "            num_layers=2,\n",
    "            batch_first=True,)\n",
    "            # bidirectional=True)\n",
    "        # self.fc1 = nn.Linear(512, 1024)\n",
    "        # self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, self.out_num)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        # self.sig = nn.Sigmoid()\n",
    "        # self.bn1 = nn.BatchNorm1d(1024)\n",
    "        # self.bn2 = nn.BatchNorm1d(512)\n",
    "        # self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def attention_net(self, x, query, mask=None):\n",
    "        d_k = query.size(-1)  # d_k为query的维度\n",
    "\n",
    "        # query:[batch, seq_len, hidden_dim*2], x.t:[batch, hidden_dim*2, seq_len]\n",
    "        #         print(\"query: \", query.shape, x.transpose(1, 2).shape)  # torch.Size([128, 38, 128]) torch.Size([128, 128, 38])\n",
    "        # 打分机制 scores: [batch, seq_len, seq_len]\n",
    "        scores = torch.matmul(query, x.transpose(1, 2)) / math.sqrt(d_k)\n",
    "        #         print(\"score: \", scores.shape)  # torch.Size([128, 38, 38])\n",
    "\n",
    "        # 对最后一个维度 归一化得分\n",
    "        alpha_n = F.softmax(scores, dim=-1)\n",
    "        #         print(\"alpha_n: \", alpha_n.shape)    # torch.Size([128, 38, 38])\n",
    "        # 对权重化的x求和\n",
    "        # [batch, seq_len, seq_len]·[batch,seq_len, hidden_dim*2] = [batch,seq_len,hidden_dim*2] -> [batch, hidden_dim*2]\n",
    "        context = torch.matmul(alpha_n, x).sum(1)\n",
    "\n",
    "        return context, alpha_n\n",
    "\n",
    "    def forward(self, x):\n",
    "        # bs = len(x)\n",
    "        # length = np.array([t.shape[0] for t in x])\n",
    "        #\n",
    "        # x, orderD = pack_sequences(x)\n",
    "        # print(self.matrix[0],self.matrix[1],self.matrix[2])\n",
    "        x = x.to(device)\n",
    "        x = self.matrix[0] * x[:, 0, :, :] + self.matrix[1] * x[:, 1, :, :] + self.matrix[2] * x[:, 2, :, :]\n",
    "        x = self.fc(x.to(device)).to(device)\n",
    "        # changed_length1 = length[orderD]\n",
    "        # x = pack_padded_sequence(x, changed_length1, batch_first=True)\n",
    "\n",
    "        out,(h_n, c_n) = self.lstm(x.to(device))     #h_state是之前的隐层状态\n",
    "        # out = torch.cat((h_n[-1, :, :], h_n[-2, :, :]), dim=-1)\n",
    "        # out1 = unpack_sequences(rnn_out, orderD)\n",
    "        # for i in range(bs):\n",
    "        #     out1[i,length[i]:-1,:] = 0\n",
    "\n",
    "        if self.att:\n",
    "            query = self.dropout(out)\n",
    "\n",
    "            # 加入attention机制\n",
    "            out, alpha_n = self.attention_net(out, query)\n",
    "\n",
    "        else:\n",
    "            out = torch.mean(out,dim=1).squeeze().cuda()\n",
    "            # out = out[:,-1,:]\n",
    "\n",
    "\n",
    "        #进行全连接\n",
    "        # out = self.fc1(out[:,-1,:])\n",
    "        # out = F.relu(out)\n",
    "        # out = self.bn1(F.dropout(out, p=0.3))\n",
    "        # out = self.fc2(out)\n",
    "        # out = F.relu(out)\n",
    "        # out = self.bn2(F.dropout(out, p=0.3))\n",
    "        out = self.fc3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc4(out)\n",
    "        # return F.softmax(out,dim=-1)\n",
    "        return out\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, compound, y, smi):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.compound = compound\n",
    "        # self.compound = torch.FloatTensor(compound)\n",
    "        # self.y = torch.FloatTensor(y)\n",
    "        self.y = y\n",
    "        self.smi = smi\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.compound[item], self.y[item], self.smi[item]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.compound)\n",
    "\n",
    "def split_multi_label(x, y, smi, k_fold, name):\n",
    "    y = np.array(y).astype(float)\n",
    "    all_smi = np.array(smi)\n",
    "    # save_path = 'tox/'+str(k_fold)+'-fold-index.pkl'\n",
    "    # if os.path.isfile(save_path):\n",
    "    #     index = joblib.load(save_path)\n",
    "    #     train_split_x = x[index[\"train_index\"]]\n",
    "    #     train_split_y = y[index[\"train_index\"]]\n",
    "    #     val_split_x = x[index[\"val_index\"]]\n",
    "    #     val_split_y = y[index[\"val_index\"]]\n",
    "    #     test_split_x = x[index[\"test_index\"]]\n",
    "    #     test_split_y = y[index[\"test_index\"]]\n",
    "    #     train_weights = joblib.load('tox/train_weights.pkl')\n",
    "    #     return train_split_x, train_split_y, val_split_x, val_split_y, test_split_x, test_split_y, train_weights\n",
    "\n",
    "    kf = KFold(5, False, 100)\n",
    "    all_train_index = [[],[],[],[],[]]\n",
    "    all_train_index_weights = [[] for i in range(y.shape[1])]\n",
    "    all_val_index = [[],[],[],[],[]]\n",
    "    all_test_index = [[],[],[],[],[]]\n",
    "    for task_index in range(y.shape[-1]):\n",
    "        negative_index = np.where(y[:, task_index] == 0)[0]\n",
    "        positive_index = np.where(y[:, task_index] == 1)[0]\n",
    "        train_index = [[],[],[],[],[]]\n",
    "        val_index = [[],[],[],[],[]]\n",
    "        test_index = [[],[],[],[],[]]\n",
    "        for k, tmp in enumerate(kf.split(negative_index)):\n",
    "            # train_tmp is  the index ofnegative_index\n",
    "            train_tmp, test_tmp = tmp\n",
    "            train_index[k].extend(negative_index[train_tmp])\n",
    "            num_t = int(len(test_tmp)/2)\n",
    "            val_index[k].extend(negative_index[test_tmp[:num_t]])\n",
    "            test_index[k].extend(negative_index[test_tmp[num_t:]])\n",
    "        for k, tmp in enumerate(kf.split(positive_index)):\n",
    "            train_tmp, test_tmp = tmp\n",
    "            train_index[k].extend(positive_index[train_tmp])\n",
    "            num_t = int(len(test_tmp)/2)\n",
    "            val_index[k].extend(positive_index[test_tmp[:num_t]])\n",
    "            test_index[k].extend(positive_index[test_tmp[num_t:]])\n",
    "\n",
    "        all_train_index_weights[task_index] = [(len(negative_index) + len(positive_index)) / len(negative_index),\n",
    "                                               (len(negative_index) + len(positive_index)) / len(positive_index)]\n",
    "\n",
    "        if task_index == 0:\n",
    "            all_train_index = train_index\n",
    "            all_val_index = val_index\n",
    "            all_test_index = test_index\n",
    "        else:\n",
    "            all_train_index = [list(set(all_train_index[i]).union(set(t))) for i, t in enumerate(train_index)]\n",
    "            all_val_index = [list(set(all_val_index[i]).union(set(t))) for i, t in enumerate(val_index)]\n",
    "            all_test_index = [list(set(all_test_index[i]).union(set(t))) for i, t in enumerate(test_index)]\n",
    "    for i in range(5):\n",
    "        joblib.dump({\"train_index\":all_train_index[i],\n",
    "                     \"val_index\": all_val_index[i],\n",
    "                     \"test_index\": all_test_index[i],\n",
    "                     }, name+'/'+str(i+1)+'-fold-index.pkl')\n",
    "    joblib.dump(all_train_index_weights, name+'/weights.pkl')\n",
    "    train_split_x = x[all_train_index[k_fold]]\n",
    "    train_split_y = y[all_train_index[k_fold]]\n",
    "    train_split_smi = all_smi[all_train_index[k_fold]]\n",
    "    val_split_x = x[all_val_index[k_fold]]\n",
    "    val_split_y = y[all_val_index[k_fold]]\n",
    "    val_split_smi = all_smi[all_val_index[k_fold]]\n",
    "    test_split_x = x[all_test_index[k_fold]]\n",
    "    test_split_y = y[all_test_index[k_fold]]\n",
    "    test_split_smi = all_smi[all_test_index[k_fold]]\n",
    "    return train_split_x, train_split_y, train_split_smi,\\\n",
    "           val_split_x, val_split_y, val_split_smi,\\\n",
    "           test_split_x, test_split_y, test_split_smi, all_train_index_weights\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置超参数\n",
    "    input_size = 512\n",
    "    hidden_size = 512  # 定义超参数rnn的循环神经元个数，个数为32个\n",
    "    learning_rate = 0.01  # 定义超参数学习率\n",
    "    epoch_num = 2000\n",
    "    batch_size = 128\n",
    "    best_loss = 10000\n",
    "    test_best_loss = 10000\n",
    "    weight_decay = 1e-5\n",
    "    momentum = 0.9\n",
    "\n",
    "    b = 0.2\n",
    "    dict_label = {\"NR-AR\": 0,\n",
    "                  \"NR-AR-LBD\": 1,\n",
    "                  \"NR-AhR\": 2,\n",
    "                  \"NR-Aromatase\": 3,\n",
    "                  \"NR-ER\": 4,\n",
    "                  \"NR-ER-LBD\": 5,\n",
    "                  \"NR-PPAR-gamma\": 6,\n",
    "                  \"SR-ARE\": 7,\n",
    "                  \"SR-ATAD5\": 8,\n",
    "                  \"SR-HSE\": 9,\n",
    "                  \"SR-MMP\": 10,\n",
    "                  \"SR-p53\": 11, }\n",
    "    tasks = list(dict_label.keys())\n",
    "    tasks_num = len(tasks)\n",
    "\n",
    "    seed = 188\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    y = joblib.load(\"tox/label.pkl\")\n",
    "    y = np.array(y).astype(float)\n",
    "    print(y.shape)\n",
    "    all_smi = joblib.load(\"tox/smi.pkl\")\n",
    "\n",
    "    x = joblib.load(\"tox/tox_embed.pkl\")\n",
    "\n",
    "    # 5-Fold\n",
    "    train_split_x, train_split_y, train_split_smi, \\\n",
    "    val_split_x, val_split_y, val_split_smi, \\\n",
    "    test_split_x, test_split_y, test_split_smi, weights = split_multi_label(x, y, all_smi, 3, 'tox')\n",
    "\n",
    "    data_train = MyDataset(train_split_x, train_split_y, train_split_smi)\n",
    "    dataset_train = data.DataLoader(dataset=data_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    data_val = MyDataset(val_split_x, val_split_y, val_split_smi)\n",
    "    dataset_val = data.DataLoader(dataset=data_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    data_test = MyDataset(test_split_x, test_split_y, test_split_smi)\n",
    "    dataset_test = data.DataLoader(dataset=data_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    rnn = LSTM(tasks_num, task_type=\"muti\", input_size=300).to(device)\n",
    "    # 设置优化器和损失函数\n",
    "    #使用adam优化器进行优化，输入待优化参数rnn.parameters，优化学习率为learning_rate\n",
    "    optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate, weight_decay=weight_decay,\n",
    "                                momentum=momentum)\n",
    "    # optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.Adadelta(rnn.parameters(), lr=learning_rate, weight_decay = weight_decay, rho=0.9)\n",
    "    # optimizer = torch.optim.RMSprop(rnn.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "    # loss_function = F.cross_entropy\n",
    "    # loss_function = F.nll_loss\n",
    "    loss_function = [nn.CrossEntropyLoss(torch.Tensor(weight).to(device), reduction='mean') for weight in weights]\n",
    "    # loss_function = nn.BCELoss()\n",
    "    # loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # 按照以下的过程进行参数的训练\n",
    "    for epoch in range(epoch_num):\n",
    "        avg_loss = 0\n",
    "        sum_loss = 0\n",
    "        rnn.train()\n",
    "        y_true_task = {}\n",
    "        y_pred_task = {}\n",
    "        y_pred_task_score = {}\n",
    "        for index, tmp in enumerate(dataset_train):\n",
    "            tmp_compound, tmp_y, tmp_smi = tmp\n",
    "            # tmp_y = tmp_y.float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = rnn(tmp_compound.to(device))\n",
    "            loss = 0\n",
    "            for i in range(len(tasks)):\n",
    "                validId = np.where((tmp_y[:, i].cpu().numpy() == 0) | (tmp_y[:, i].cpu().numpy() == 1))[0]\n",
    "                if len(validId) == 0:\n",
    "                    continue\n",
    "                y_pred = outputs[:, i * 2:(i + 1) * 2][torch.tensor(validId).to(device)]\n",
    "                y_label = tmp_y[:, i][torch.tensor(validId).to(device)]\n",
    "\n",
    "                # y_pred = torch.sigmoid(y_pred).view(-1)\n",
    "                # y_label = F.one_hot(y_label, 2).float().to(device)\n",
    "                loss += loss_function[i](y_pred.to(device), y_label.long().to(device))\n",
    "\n",
    "                pred_lable = F.softmax(y_pred.detach().cpu(), dim=-1)[:, 1].view(-1).numpy()\n",
    "                # pred_lable = np.zeros_like(y_pred.cpu().detach().numpy(), dtype=int)\n",
    "                # pred_lable[np.where(np.asarray(y_pred.cpu().detach().numpy()) > 0.5)] = 1\n",
    "                try:\n",
    "                    y_true_task[i].extend(y_label.cpu().numpy())\n",
    "                    y_pred_task[i].extend(pred_lable)\n",
    "                    # y_pred_task_score[i].extend(y_pred)\n",
    "                except:\n",
    "                    y_true_task[i] = []\n",
    "                    y_pred_task[i] = []\n",
    "                    # y_pred_task_score[i] = []\n",
    "                    y_true_task[i].extend(y_label.cpu().numpy())\n",
    "                    y_pred_task[i].extend(pred_lable)\n",
    "                    # y_pred_task_score[i].extend(y_pred.cpu().detach().numpy())\n",
    "\n",
    "                # flood = (loss - b).abs() + b\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss += loss\n",
    "            # print(\"epoch:\", epoch, \"index: \", index,\"loss:\", loss.item())\n",
    "        avg_loss = sum_loss / (index + 1)\n",
    "        # cm = [metrics.confusion_matrix(y_true_task[i], y_pred_task[i]) for i in range(len(tasks))]\n",
    "        trn_roc = [metrics.roc_auc_score(y_true_task[i], y_pred_task[i]) for i in range(len(tasks))]\n",
    "        trn_prc = [metrics.auc(precision_recall_curve(y_true_task[i], y_pred_task[i])[1],\n",
    "                               precision_recall_curve(y_true_task[i], y_pred_task[i])[0]) for i in range(len(tasks))]\n",
    "        # acc = [metrics.accuracy_score(y_true_task[i], y_pred_task[i]) for i in range(len(tasks))]\n",
    "        # recall = [metrics.recall_score(y_true_task[i], y_pred_task[i]) for i in range(len(tasks))]\n",
    "        # specificity = [cm[i][0, 0] / (cm[i][0, 0] + cm[i][0, 1]) for i in range(len(tasks))]\n",
    "\n",
    "        print(\"epoch:\", epoch, \"   train  \"  \"avg_loss:\", avg_loss.item(),\n",
    "              # \"acc: \", np.array(acc).mean(),\n",
    "              # \"recall: \", np.array(recall).mean(),\n",
    "              # \"specificity: \", np.array(specificity).mean(),\n",
    "              \" train_auc: \", np.array(trn_roc).mean(),\n",
    "              \" train_pr: \", np.array(trn_prc).mean())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rnn.eval()\n",
    "            val_sum_loss = []\n",
    "            y_true_task = {}\n",
    "            y_pred_task = {}\n",
    "            y_pred_task_score = {}\n",
    "            for index, tmp in enumerate(dataset_val):\n",
    "                tmp_compound, tmp_y, tmp_smi = tmp\n",
    "                loss = 0\n",
    "                outputs = rnn(tmp_compound)\n",
    "                # out_label = F.softmax(outputs, dim=1)\n",
    "                # pred = out_label.data.max(1, keepdim=True)[1].view(-1).cpu().numpy()\n",
    "                # pred_score = [x[tmp_y.cpu().detach().numpy()[i]] for i, x in enumerate(out_label.cpu().detach().numpy())]\n",
    "                # y_pred.extend(pred)\n",
    "                # y_pred_score.extend(pred_score)\n",
    "                for i in range(tasks_num):\n",
    "                    validId = np.where((tmp_y[:, i].cpu().numpy() == 0) | (tmp_y[:, i].cpu().numpy() == 1))[0]\n",
    "                    if len(validId) == 0:\n",
    "                        continue\n",
    "                    y_pred = outputs[:, i * 2:(i + 1) * 2][torch.tensor(validId)].to(device)\n",
    "                    y_label = tmp_y[:, i][torch.tensor(validId)].long().to(device)\n",
    "\n",
    "                    # y_pred = torch.sigmoid(y_pred).view(-1)\n",
    "                    # y_label = F.one_hot(y_label, 2).float().to(device)\n",
    "                    loss += loss_function[i](y_pred, y_label)\n",
    "\n",
    "                    pred_lable = F.softmax(y_pred.detach().cpu(), dim=-1)[:, 1].view(-1).numpy()\n",
    "                    # pred_lable = np.zeros_like(y_pred.cpu().detach().numpy(), dtype=int)\n",
    "                    # pred_lable[np.where(np.asarray(y_pred.cpu().detach().numpy()) > 0.5)] = 1\n",
    "                    try:\n",
    "                        y_true_task[i].extend(y_label.cpu().numpy())\n",
    "                        y_pred_task[i].extend(pred_lable)\n",
    "                        # y_pred_task_score[i].extend(y_pred)\n",
    "                    except:\n",
    "                        y_true_task[i] = []\n",
    "                        y_pred_task[i] = []\n",
    "                        # y_pred_task_score[i] = []\n",
    "                        y_true_task[i].extend(y_label.cpu().numpy())\n",
    "                        y_pred_task[i].extend(pred_lable)\n",
    "                        # y_pred_task_score[i].extend(y_pred.cpu().detach().numpy())\n",
    "\n",
    "                val_sum_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "            val_avg_loss = np.array(val_sum_loss).mean()\n",
    "\n",
    "            trn_roc = [metrics.roc_auc_score(y_true_task[i], y_pred_task[i]) for i in range(tasks_num)]\n",
    "            trn_prc = [metrics.auc(precision_recall_curve(y_true_task[i], y_pred_task[i])[1],\n",
    "                                   precision_recall_curve(y_true_task[i], y_pred_task[i])[0]) for i in\n",
    "                       range(tasks_num)]\n",
    "            # acc = [metrics.accuracy_score(y_true_task[i], y_pred_task[i]) for i in range(tasks_num)]\n",
    "            # recall = [metrics.recall_score(y_true_task[i], y_pred_task[i]) for i in range(tasks_num)]\n",
    "            # specificity = [cm[i][0, 0] / (cm[i][0, 0] + cm[i][0, 1]) for i in range(tasks_num)]\n",
    "\n",
    "            print(\"epoch:\", epoch, \"   val  \"  \"avg_loss:\", val_avg_loss,\n",
    "                  # \"acc: \", np.array(acc).mean(),\n",
    "                  # \"recall: \", np.array(recall).mean(),\n",
    "                  # \"specificity: \", np.array(specificity).mean(),\n",
    "                  # \" val_auc: \", trn_roc,\n",
    "                  \" val_auc: \", np.array(trn_roc).mean(),\n",
    "                  # \" val_pr: \", trn_prc,\n",
    "                  \" val_pr: \", np.array(trn_prc).mean())\n",
    "\n",
    "            # 保存模型\n",
    "            if val_avg_loss < test_best_loss:\n",
    "                test_best_loss = val_avg_loss\n",
    "                PATH = 'tox/lstm_net.pth'\n",
    "                print(\"test save model\")\n",
    "                torch.save(rnn.state_dict(), PATH)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    rnn.eval()\n",
    "                    test_sum_loss = []\n",
    "                    y_true_task = {}\n",
    "                    y_pred_task = {}\n",
    "                    y_pred_task_score = {}\n",
    "                    for index, tmp in enumerate(dataset_test):\n",
    "                        tmp_compound, tmp_y, tmp_smi = tmp\n",
    "                        loss = 0\n",
    "                        outputs = rnn(tmp_compound)\n",
    "                        # out_label = F.softmax(outputs, dim=1)\n",
    "                        # pred = out_label.data.max(1, keepdim=True)[1].view(-1).cpu().numpy()\n",
    "                        # pred_score = [x[tmp_y.cpu().detach().numpy()[i]] for i, x in enumerate(out_label.cpu().detach().numpy())]\n",
    "                        # y_pred.extend(pred)\n",
    "                        # y_pred_score.extend(pred_score)\n",
    "                        for i in range(tasks_num):\n",
    "                            validId = np.where((tmp_y[:, i].cpu().numpy() == 0) | (tmp_y[:, i].cpu().numpy() == 1))[0]\n",
    "                            if len(validId) == 0:\n",
    "                                continue\n",
    "                            y_pred = outputs[:, i * 2:(i + 1) * 2][torch.tensor(validId)].to(device)\n",
    "                            y_label = tmp_y[:, i][torch.tensor(validId)].long().to(device)\n",
    "\n",
    "                            # y_pred = torch.sigmoid(y_pred).view(-1)\n",
    "                            # y_label = F.one_hot(y_label, 2).float().to(device)\n",
    "                            loss += loss_function[i](y_pred, y_label)\n",
    "\n",
    "                            y_pred_s = F.softmax(y_pred.detach().cpu(), dim=-1)[:, 1].view(-1).numpy()\n",
    "\n",
    "                            pred_lable = np.zeros_like(y_pred_s, dtype=int)\n",
    "                            pred_lable[np.where(np.asarray(y_pred_s) > 0.5)] = 1\n",
    "                            try:\n",
    "                                y_true_task[i].extend(y_label.cpu().numpy())\n",
    "                                y_pred_task[i].extend(pred_lable)\n",
    "                                y_pred_task_score[i].extend(y_pred_s)\n",
    "                            except:\n",
    "                                y_true_task[i] = []\n",
    "                                y_pred_task[i] = []\n",
    "                                y_pred_task_score[i] = []\n",
    "\n",
    "                                y_true_task[i].extend(y_label.cpu().numpy())\n",
    "                                y_pred_task[i].extend(pred_lable)\n",
    "                                y_pred_task_score[i].extend(y_pred_s)\n",
    "\n",
    "                        test_sum_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "                    trn_roc = [metrics.roc_auc_score(y_true_task[i], y_pred_task_score[i]) for i in range(tasks_num)]\n",
    "                    trn_prc = [metrics.auc(precision_recall_curve(y_true_task[i], y_pred_task_score[i])[1],\n",
    "                                           precision_recall_curve(y_true_task[i], y_pred_task_score[i])[0]) for i in\n",
    "                               range(tasks_num)]\n",
    "                    # print(len(trn_roc))\n",
    "                    # print(sum(y_true_task[0]))\n",
    "                    # print(sum(y_pred_task[0]))\n",
    "                    acc = [metrics.accuracy_score(y_true_task[i], y_pred_task[i]) for i in range(tasks_num)]\n",
    "                    # recall = [metrics.recall_score(y_true_task[i], y_pred_task[i]) for i in range(tasks_num)]\n",
    "                    # specificity = [cm[i][0, 0] / (cm[i][0, 0] + cm[i][0, 1]) for i in range(tasks_num)]\n",
    "\n",
    "                    print(\"epoch:\", epoch, \"   test  \"  \"avg_loss:\", np.array(test_sum_loss).mean(),\n",
    "                          \"acc: \", np.array(acc).mean(),\n",
    "                          # \"recall: \", np.array(recall).mean(),\n",
    "                          # \"specificity: \", np.array(specificity).mean(),\n",
    "                          # \" test_auc: \", trn_roc,\n",
    "                          \" test_auc: \", np.array(trn_roc).mean(),\n",
    "                          # \" test_pr: \", trn_prc,\n",
    "                          \" test_pr: \", np.array(trn_prc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f385d0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9015d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
